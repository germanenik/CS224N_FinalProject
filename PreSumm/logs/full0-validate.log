[2021-03-10 19:41:22,765 INFO] Loading checkpoint from ../full0_models/model_step_23000.pt
[2021-03-10 19:52:15,983 INFO] Loading checkpoint from ../full0_models/model_step_23000.pt
[2021-03-10 19:52:59,277 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2021-03-10 19:52:59,290 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-03-10 19:52:59,319 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2021-03-10 19:53:42,909 INFO] Loading checkpoint from ../full0_models/model_step_23000.pt
[2021-03-10 19:53:43,674 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2021-03-10 19:53:43,675 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-03-10 19:53:43,709 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2021-03-10 19:54:00,515 INFO] Loading valid dataset from ../full0_data2/valid.0.bert.pt, number of examples: 1541
[2021-03-10 19:54:00,518 INFO] * number of parameters: 120512513
[2021-03-10 19:55:46,297 INFO] Validation xent: 14.8576 at step 23000
[2021-03-10 19:55:46,322 INFO] Loading checkpoint from ../full0_models/model_step_23000.pt
[2021-03-10 19:55:47,040 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2021-03-10 19:55:47,040 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2021-03-10 19:55:47,075 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2021-03-10 19:55:50,749 INFO] Loading test dataset from ../full0_data2/test.0.bert.pt, number of examples: 1718
[2021-03-10 19:55:50,752 INFO] * number of parameters: 120512513
[2021-03-10 19:57:59,308 INFO] Writing summaries.
[2021-03-10 19:57:59,308 INFO] Processing summaries. Saving system files to ../temp/tmpwxge1b3f/system and model files to ../temp/tmpwxge1b3f/model.
[2021-03-10 19:57:59,308 INFO] Processing files in ../temp/rouge-tmp-2021-03-10-19-57-59/candidate/.
[2021-03-10 19:57:59,496 INFO] Saved processed files to ../temp/tmpwxge1b3f/system.
[2021-03-10 19:57:59,497 INFO] Processing files in ../temp/rouge-tmp-2021-03-10-19-57-59/reference/.
[2021-03-10 19:57:59,723 INFO] Saved processed files to ../temp/tmpwxge1b3f/model.
[2021-03-10 19:57:59,737 INFO] Written ROUGE configuration to ../temp/tmpdllq88oe/rouge_conf.xml
[2021-03-10 19:57:59,737 INFO] Running ROUGE with command /home/legalese/pyrouge/rouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/legalese/pyrouge/rouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpdllq88oe/rouge_conf.xml
[2021-03-10 19:58:55,544 INFO] Rouges at step 23000 
>> ROUGE-F(1/2/3/l): 19.44/5.00/16.95
ROUGE-R(1/2/3/l): 18.80/4.41/16.13

[2021-03-10 19:58:55,545 INFO] Validation xent: 13.4664 at step 23000
